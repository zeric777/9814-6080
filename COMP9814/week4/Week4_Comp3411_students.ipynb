{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6cbc7df1-2c8f-48de-ac96-d5b3a8db5b7f",
   "metadata": {},
   "source": [
    "# Defining the environment and the required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c85e5664-c640-4ee1-817c-b5011c71b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class World(object):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.R = np.zeros(self.x*self.y)\n",
    "        self.agentPos = 0\n",
    "        self.grid = np.zeros((self.x, self.y))  # Adding the grid so we can visualize the environment later\n",
    "        self.goalState = None  # Goal state will be set when calling setReward()\n",
    "        self.fearState= None\n",
    "\n",
    "    def xy2idx(self,x,y): # transfering the location to a unique id representing the current state\n",
    "        return x*self.y + y\n",
    "\n",
    "    def idx2xy(self,idx):\n",
    "        # To do\n",
    "        x = int(idx / self.y)\n",
    "        y = idx % self.y\n",
    "        # Based on idx that we have, how we can retrieve x, y?\n",
    "        return x, y\n",
    "        \n",
    "\n",
    "    def resetAgent(self, pos):\n",
    "        self.agentPos = int(pos)\n",
    "\n",
    "    def setReward(self, x, y, r):\n",
    "        # get the goal state location, and assigne a reward to that cell\n",
    "        goalState = self.xy2idx(x, y)\n",
    "        self.R[goalState] = r\n",
    "        if r> 0:\n",
    "            self.goalState = (x, y)  # Store goal state as a tuple (x, y)\n",
    "        else:\n",
    "            self.fearState= (x,y)\n",
    "\n",
    "    def getState(self):\n",
    "        return self.agentPos\n",
    "\n",
    "    def getReward(self):\n",
    "        return self.R[self.agentPos]\n",
    "\n",
    "    def getNumOfStates(self):\n",
    "        return self.x*self.y\n",
    "\n",
    "    def getNumOfActions(self):\n",
    "        return 4\n",
    "\n",
    "    def move(self,id):\n",
    "        x_, y_ = self.idx2xy(self.agentPos)\n",
    "        tmpX = x_\n",
    "        tmpY = y_\n",
    "        \n",
    "        # To do \n",
    "        # based on each possible action (id), change tmpX and tmpY\n",
    "\n",
    "        if self.validMove(tmpX, tmpY):\n",
    "            self.agentPos = self.xy2idx(tmpX,tmpY)\n",
    "\n",
    "    def validMove(self,x,y):\n",
    "        valid = True\n",
    "        # To do\n",
    "        valid = True\n",
    "        # Add situations that Move will be invalid and Valid= False        \n",
    "        return valid\n",
    "\n",
    "\n",
    "    def display(self):\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "        ax.set_facecolor('white')\n",
    "        \n",
    "        ax.imshow(self.grid, cmap=\"viridis\", origin=\"upper\", extent=(0, self.y, 0, self.x), alpha=0)\n",
    "        agent_x, agent_y = self.idx2xy(self.agentPos)  \n",
    "        agent_circle = plt.Circle((agent_y + 0.5, self.x - agent_x - 0.5), 0.3, color='grey', ec='black') \n",
    "        ax.add_patch(agent_circle) \n",
    "    \n",
    "\n",
    "        if hasattr(self, 'goalState'):\n",
    "            goal_x, goal_y = self.goalState \n",
    "            goal_circle = plt.Circle((goal_y + 0.5, self.x - goal_x - 0.5), 0.3, color='green', ec='black') \n",
    "            ax.add_patch(goal_circle)  \n",
    "    \n",
    " \n",
    "        if self.fearState != None:\n",
    "            if hasattr(self, 'fearState'):\n",
    "                fear_x, fear_y = self.fearState  \n",
    "                fear_circle = plt.Circle((fear_y + 0.5, self.x - fear_x - 0.5), 0.3, color='red', ec='black')  \n",
    "                ax.add_patch(fear_circle) \n",
    "        \n",
    "        ax.set_xticks(np.arange(self.y))\n",
    "        ax.set_yticks(np.arange(self.x))\n",
    "        ax.set_xticklabels(np.arange(self.y))\n",
    "        ax.set_yticklabels(np.arange(self.x)[::-1])\n",
    "        ax.grid(which=\"both\", color=\"black\", linestyle=\"-\", linewidth=1)\n",
    "    \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "475a3d5f-a3de-453c-a55a-643ba198676a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m world\u001b[38;5;241m.\u001b[39msetReward(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;66;03m#Goal state\u001b[39;00m\n\u001b[1;32m      3\u001b[0m world\u001b[38;5;241m.\u001b[39msetReward(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;66;03m#Fear region\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m world\u001b[38;5;241m.\u001b[39mdisplay()\n",
      "Cell \u001b[0;32mIn[1], line 72\u001b[0m, in \u001b[0;36mWorld.display\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_facecolor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     71\u001b[0m ax\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m\"\u001b[39m, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m\"\u001b[39m, extent\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx), alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m agent_x, agent_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx2xy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magentPos)  \n\u001b[1;32m     73\u001b[0m agent_circle \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mCircle((agent_y \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m-\u001b[39m agent_x \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m), \u001b[38;5;241m0.3\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrey\u001b[39m\u001b[38;5;124m'\u001b[39m, ec\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m     74\u001b[0m ax\u001b[38;5;241m.\u001b[39madd_patch(agent_circle) \n",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m, in \u001b[0;36mWorld.idx2xy\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21midx2xy\u001b[39m(\u001b[38;5;28mself\u001b[39m,idx):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# To do\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Based on idx that we have, how we can retrieve x, y?\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAisAAAGiCAYAAAAm+YalAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdsElEQVR4nO3db2zV13348c8NBjtbsVtIMUY4iWkjNyVKRu1sOIrpH2+OoIoSKQ/2oEro1kp1RULBQktMHlTbHrjbopZGTWCokCqLukSbIUMirbBUbFOVSHVmkiwlNNsItpA9xtbYKdvswL6/B/1xNcfGcI0Nx9evl/R9cL+c43uOjoTfuvfazmVZlgUAQKJuuN4LAACYilgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAklZQrOzcuTPuvPPOKC8vj/Ly8mhoaIgf/ehHU87p7u6Ourq6KCsri1WrVsWuXbuuasEAwPxSUKysXLkyvvWtb0Vvb2/09vbGF77whXjggQfirbfemnT8yZMnY8OGDdHY2Bh9fX2xffv22Lx5c3R0dMzI4gGA4pe72j9kuGTJkvirv/qr+MpXvjLh3x5//PE4cOBAHD9+PH+vpaUlXn/99Th69OjVPC0AME+UTHfihQsX4u/+7u/i3Llz0dDQMOmYo0ePRnNz87h79913X+zZsyc++OCDWLhw4aTzRkdHY3R0NP/4f//3f+M///M/Y+nSpZHL5aa7ZABglmVZFu+//36sWLEibrhhZj4aW3CsvPnmm9HQ0BD/8z//Ex/5yEdi//798elPf3rSsUNDQ1FZWTnuXmVlZZw/fz7Onj0bVVVVk85rb2+PP/3TPy10aQBAIgYGBmLlypUz8rUKjpXa2to4duxYvPfee9HR0REbN26M7u7uSwbLh18Jufiu01SvkLS1tUVra2v+8fDwcNx8880xMDAQ5eXlhS4ZALhGRkZGorq6OhYvXjxjX7PgWFm0aFF88pOfjIiI+vr6+PnPfx7f/e5346//+q8njF2+fHkMDQ2Nu3fmzJkoKSmJpUuXXvI5SktLo7S0dML9iz+FBACkbSY/tnHVbyZlWTbu8yX/V0NDQ3R2do67d+jQoaivr7/k51UAAP6vgmJl+/btceTIkXj33XfjzTffjCeffDK6urriS1/6UkT85u2bRx55JD++paUlTp06Fa2trXH8+PHYu3dv7NmzJ7Zt2zazuwAAilZBbwP927/9Wzz88MMxODgYFRUVceedd8aPf/zj+IM/+IOIiBgcHIz+/v78+JqamnjllVdi69at8cwzz8SKFSvi6aefjoceemhmdwEAFK2r/j0r18LIyEhUVFTE8PCwz6wAQMJm43u2vw0EACRNrAAASRMrAEDSxAoAkDSxAgAkTawAAEkTKwBA0sQKAJA0sQIAJE2sAABJEysAQNLECgCQNLECACRNrAAASRMrAEDSxAoAkDSxAgAkTawAAEkTKwBA0sQKAJA0sQIAJE2sAABJEysAQNLECgCQNLECACRNrAAASRMrAEDSxAoAkDSxAgAkTawAAEkTKwBA0sQKAJA0sQIAJE2sAABJEysAQNLECgCQNLECACRNrAAASRMrAEDSxAoAkDSxAgAkTawAAEkTKwBA0sQKAJA0sQIAJE2sAABJEysAQNLECgCQNLECACRNrAAASRMrAEDSCoqV9vb2uPvuu2Px4sWxbNmyePDBB+PEiRNTzunq6opcLjfhevvtt69q4QDA/FBQrHR3d8emTZvi1Vdfjc7Ozjh//nw0NzfHuXPnLjv3xIkTMTg4mL9uu+22aS8aAJg/SgoZ/OMf/3jc4+eeey6WLVsWr732Wqxbt27KucuWLYuPfvSjV/Q8o6OjMTo6mn88MjJSyDIBgCJyVZ9ZGR4ejoiIJUuWXHbsmjVroqqqKpqamuLw4cNTjm1vb4+Kior8VV1dfTXLBADmsFyWZdl0JmZZFg888ED86le/iiNHjlxy3IkTJ6Knpyfq6upidHQ0/uZv/iZ27doVXV1dl3w1ZrJXVqqrq2N4eDjKy8uns1wA4BoYGRmJioqKGf2ePe1Y2bRpUxw8eDB++tOfxsqVKwuae//990cul4sDBw5c0fjZ2DgAMPNm43v2tN4Geuyxx+LAgQNx+PDhgkMlImLt2rXxzjvvTOepAYB5pqAP2GZZFo899ljs378/urq6oqamZlpP2tfXF1VVVdOaCwDMLwXFyqZNm+KHP/xh/MM//EMsXrw4hoaGIiKioqIibrzxxoiIaGtri9OnT8fzzz8fERE7duyIW2+9NVavXh1jY2PxwgsvREdHR3R0dMzwVgCAYlRQrOzcuTMiIj73uc+Nu//cc8/Fl7/85YiIGBwcjP7+/vy/jY2NxbZt2+L06dNx4403xurVq+PgwYOxYcOGq1s5ADAvTPsDtteSD9gCwNyQzAdsAQCuFbECACRNrAAASRMrAEDSxAoAkDSxAgAkTawAAEkTKwBA0sQKAJA0sQIAJE2sAABJEysAQNLECgCQNLECACRNrAAASRMrAEDSxAoAkDSxAgAkTawAAEkTKwBA0sQKAJA0sQIAJE2sAABJEysAQNLECgCQNLECACRNrAAASRMrAEDSxAoAkDSxAgAkTawAAEkTKwBA0sQKAJA0sQIAJE2sAABJEysAQNLECgCQNLECACRNrAAASRMrAEDSxAoAkDSxAgAkTawAAEkTKwBA0sQKAJA0sQIAJE2sAABJEysAQNLECgCQNLECACStoFhpb2+Pu+++OxYvXhzLli2LBx98ME6cOHHZed3d3VFXVxdlZWWxatWq2LVr17QXDADMLwXFSnd3d2zatCleffXV6OzsjPPnz0dzc3OcO3fuknNOnjwZGzZsiMbGxujr64vt27fH5s2bo6Oj46oXDwAUv1yWZdl0J//7v/97LFu2LLq7u2PdunWTjnn88cfjwIEDcfz48fy9lpaWeP311+Po0aNX9DwjIyNRUVERw8PDUV5ePt3lAgCzbDa+Z1/VZ1aGh4cjImLJkiWXHHP06NFobm4ed+++++6L3t7e+OCDDyadMzo6GiMjI+MuAGB+mnasZFkWra2tce+998Ydd9xxyXFDQ0NRWVk57l5lZWWcP38+zp49O+mc9vb2qKioyF/V1dXTXSYAMMdNO1YeffTReOONN+Jv//ZvLzs2l8uNe3zxnacP37+ora0thoeH89fAwMB0lwkAzHEl05n02GOPxYEDB6KnpydWrlw55djly5fH0NDQuHtnzpyJkpKSWLp06aRzSktLo7S0dDpLAwCKTEGvrGRZFo8++mjs27cvfvKTn0RNTc1l5zQ0NERnZ+e4e4cOHYr6+vpYuHBhYasFAOadgmJl06ZN8cILL8QPf/jDWLx4cQwNDcXQ0FD893//d35MW1tbPPLII/nHLS0tcerUqWhtbY3jx4/H3r17Y8+ePbFt27aZ2wUAULQKipWdO3fG8PBwfO5zn4uqqqr89dJLL+XHDA4ORn9/f/5xTU1NvPLKK9HV1RW/8zu/E3/+538eTz/9dDz00EMztwsAoGhd1e9ZuVb8nhUAmBuS+z0rAACzTawAAEkTKwBA0sQKAJA0sQIAJE2sAABJEysAQNLECgCQNLECACRNrAAASRMrAEDSxAoAkDSxAgAkTawAAEkTKwBA0sQKAJA0sQIAJE2sAABJEysAQNLECgCQNLECACRNrAAASRMrAEDSxAoAkDSxAgAkTawAAEkTKwBA0sQKAJA0sQIAJE2sAABJEysAQNLECgCQNLECACRNrAAASRMrAEDSxAoAkDSxAgAkTawAAEkTKwBA0sQKAJA0sQIAJE2sAABJEysAQNLECgCQNLECACRNrAAASRMrAEDSxAoAkDSxAgAkTawAAEkTKwBA0gqOlZ6enrj//vtjxYoVkcvl4uWXX55yfFdXV+RyuQnX22+/Pd01AwDzSEmhE86dOxd33XVX/NEf/VE89NBDVzzvxIkTUV5enn/88Y9/vNCnBgDmoYJjZf369bF+/fqCn2jZsmXx0Y9+9IrGjo6OxujoaP7xyMhIwc8HABSHa/aZlTVr1kRVVVU0NTXF4cOHpxzb3t4eFRUV+au6uvoarRIASM2sx0pVVVXs3r07Ojo6Yt++fVFbWxtNTU3R09NzyTltbW0xPDycvwYGBmZ7mQBAogp+G6hQtbW1UVtbm3/c0NAQAwMD8dRTT8W6desmnVNaWhqlpaWzvTQAYA64Lj+6vHbt2njnnXeux1MDAHPMdYmVvr6+qKqquh5PDQDMMQW/DfTrX/86/vmf/zn/+OTJk3Hs2LFYsmRJ3HzzzdHW1hanT5+O559/PiIiduzYEbfeemusXr06xsbG4oUXXoiOjo7o6OiYuV0AAEWr4Fjp7e2Nz3/+8/nHra2tERGxcePG+MEPfhCDg4PR39+f//exsbHYtm1bnD59Om688cZYvXp1HDx4MDZs2DADywcAil0uy7Lsei/ickZGRqKioiKGh4fH/WI5ACAts/E9298GAgCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKQVHCs9PT1x//33x4oVKyKXy8XLL7982Tnd3d1RV1cXZWVlsWrVqti1a9d01goAzEMFx8q5c+firrvuiu9973tXNP7kyZOxYcOGaGxsjL6+vti+fXts3rw5Ojo6Cl4sADD/lBQ6Yf369bF+/forHr9r1664+eabY8eOHRERcfvtt0dvb2889dRT8dBDDxX69ADAPDPrn1k5evRoNDc3j7t33333RW9vb3zwwQeTzhkdHY2RkZFxFwAwP816rAwNDUVlZeW4e5WVlXH+/Pk4e/bspHPa29ujoqIif1VXV8/2MgGARF2TnwbK5XLjHmdZNun9i9ra2mJ4eDh/DQwMzPoaAYA0FfyZlUItX748hoaGxt07c+ZMlJSUxNKlSyedU1paGqWlpbO9NABgDpj1V1YaGhqis7Nz3L1Dhw5FfX19LFy4cLafHgCY4wqOlV//+tdx7NixOHbsWET85keTjx07Fv39/RHxm7dwHnnkkfz4lpaWOHXqVLS2tsbx48dj7969sWfPnti2bdvM7AAAKGoFvw3U29sbn//85/OPW1tbIyJi48aN8YMf/CAGBwfz4RIRUVNTE6+88kps3bo1nnnmmVixYkU8/fTTfmwZALgiuezip10TNjIyEhUVFTE8PBzl5eXXezkAwCXMxvdsfxsIAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkTStWnn322aipqYmysrKoq6uLI0eOXHJsV1dX5HK5Cdfbb7897UUDAPNHwbHy0ksvxZYtW+LJJ5+Mvr6+aGxsjPXr10d/f/+U806cOBGDg4P567bbbpv2ogGA+aPgWPn2t78dX/nKV+KrX/1q3H777bFjx46orq6OnTt3Tjlv2bJlsXz58vy1YMGCaS8aAJg/CoqVsbGxeO2116K5uXnc/ebm5vjZz3425dw1a9ZEVVVVNDU1xeHDh6ccOzo6GiMjI+MuAGB+KihWzp49GxcuXIjKyspx9ysrK2NoaGjSOVVVVbF79+7o6OiIffv2RW1tbTQ1NUVPT88ln6e9vT0qKiryV3V1dSHLBACKSMl0JuVyuXGPsyybcO+i2traqK2tzT9uaGiIgYGBeOqpp2LdunWTzmlra4vW1tb845GREcECAPNUQa+s3HTTTbFgwYIJr6KcOXNmwqstU1m7dm288847l/z30tLSKC8vH3cBAPNTQbGyaNGiqKuri87OznH3Ozs745577rnir9PX1xdVVVWFPDUAME8V/DZQa2trPPzww1FfXx8NDQ2xe/fu6O/vj5aWloj4zVs4p0+fjueffz4iInbs2BG33nprrF69OsbGxuKFF16Ijo6O6OjomNmdAABFqeBY+cM//MP4j//4j/izP/uzGBwcjDvuuCNeeeWVuOWWWyIiYnBwcNzvXBkbG4tt27bF6dOn48Ybb4zVq1fHwYMHY8OGDTO3CwCgaOWyLMuu9yIuZ2RkJCoqKmJ4eNjnVwAgYbPxPdvfBgIAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGliBQBImlgBAJImVgCApIkVACBpYgUASJpYAQCSJlYAgKSJFQAgaWIFAEiaWAEAkiZWAICkiRUAIGnTipVnn302ampqoqysLOrq6uLIkSNTju/u7o66urooKyuLVatWxa5du6a1WABg/ik4Vl566aXYsmVLPPnkk9HX1xeNjY2xfv366O/vn3T8yZMnY8OGDdHY2Bh9fX2xffv22Lx5c3R0dFz14gGA4pfLsiwrZMLv/d7vxWc+85nYuXNn/t7tt98eDz74YLS3t08Y//jjj8eBAwfi+PHj+XstLS3x+uuvx9GjRyd9jtHR0RgdHc0/Hh4ejptvvjkGBgaivLy8kOUCANfQyMhIVFdXx3vvvRcVFRUz80WzAoyOjmYLFizI9u3bN+7+5s2bs3Xr1k06p7GxMdu8efO4e/v27ctKSkqysbGxSed885vfzCLC5XK5XC7XHL3+5V/+pZDEmFJJFODs2bNx4cKFqKysHHe/srIyhoaGJp0zNDQ06fjz58/H2bNno6qqasKctra2aG1tzT9+77334pZbbon+/v6Zq7TEXSzT+fZqkn3Pn33Pxz1HzM99z8c9R8zffV98N2TJkiUz9jULipWLcrncuMdZlk24d7nxk92/qLS0NEpLSyfcr6iomFcHHhFRXl4+7/YcYd/zyXzcc8T83Pd83HPE/N33DTfM3A8cF/SVbrrppliwYMGEV1HOnDkz4dWTi5YvXz7p+JKSkli6dGmBywUA5puCYmXRokVRV1cXnZ2d4+53dnbGPffcM+mchoaGCeMPHToU9fX1sXDhwgKXCwDMNwW/RtPa2hrf//73Y+/evXH8+PHYunVr9Pf3R0tLS0T85vMmjzzySH58S0tLnDp1KlpbW+P48eOxd+/e2LNnT2zbtu2Kn7O0tDS++c1vTvrWULGaj3uOsO/5tO/5uOeI+bnv+bjnCPueyX0X/KPLEb/5pXB/+Zd/GYODg3HHHXfEd77znVi3bl1ERHz5y1+Od999N7q6uvLju7u7Y+vWrfHWW2/FihUr4vHHH8/HDQDAVKYVKwAA14q/DQQAJE2sAABJEysAQNLECgCQtGRi5dlnn42ampooKyuLurq6OHLkyJTju7u7o66uLsrKymLVqlWxa9eua7TSmVPInru6uiKXy0243n777Wu44qvX09MT999/f6xYsSJyuVy8/PLLl50z18+60D0Xw1m3t7fH3XffHYsXL45ly5bFgw8+GCdOnLjsvLl+1tPZ91w/7507d8add96Z/y2tDQ0N8aMf/WjKOXP9nCMK3/dcP+fJtLe3Ry6Xiy1btkw5bibOO4lYeemll2LLli3x5JNPRl9fXzQ2Nsb69eujv79/0vEnT56MDRs2RGNjY/T19cX27dtj8+bN0dHRcY1XPn2F7vmiEydOxODgYP667bbbrtGKZ8a5c+firrvuiu9973tXNL4YzrrQPV80l8+6u7s7Nm3aFK+++mp0dnbG+fPno7m5Oc6dO3fJOcVw1tPZ90Vz9bxXrlwZ3/rWt6K3tzd6e3vjC1/4QjzwwAPx1ltvTTq+GM45ovB9XzRXz/nDfv7zn8fu3bvjzjvvnHLcjJ33jP1JxKvwu7/7u1lLS8u4e5/61KeyJ554YtLxf/Inf5J96lOfGnfva1/7WrZ27dpZW+NMK3TPhw8fziIi+9WvfnUNVndtRES2f//+KccUw1n/X1ey52I86zNnzmQRkXV3d19yTLGddZZd2b6L8bw/9rGPZd///vcn/bdiPOeLptp3MZ3z+++/n912221ZZ2dn9tnPfjb7xje+ccmxM3Xe1/2VlbGxsXjttdeiubl53P3m5ub42c9+Numco0ePThh/3333RW9vb3zwwQezttaZMp09X7RmzZqoqqqKpqamOHz48GwuMwlz/ayvRjGd9fDwcETElH+FtRjP+kr2fVExnPeFCxfixRdfjHPnzkVDQ8OkY4rxnK9k3xcVwzlv2rQpvvjFL8bv//7vX3bsTJ33dY+Vs2fPxoULFyb8IcTKysoJfwDxoqGhoUnHnz9/Ps6ePTtra50p09lzVVVV7N69Ozo6OmLfvn1RW1sbTU1N0dPTcy2WfN3M9bOejmI76yzLorW1Ne6999644447Ljmu2M76SvddDOf95ptvxkc+8pEoLS2NlpaW2L9/f3z605+edGwxnXMh+y6Gc46IePHFF+Mf//Efo729/YrGz9R5lxS0ylmUy+XGPc6ybMK9y42f7H7KCtlzbW1t1NbW5h83NDTEwMBAPPXUU/k/dVCsiuGsC1FsZ/3oo4/GG2+8ET/96U8vO7aYzvpK910M511bWxvHjh2L9957Lzo6OmLjxo3R3d19yW/cxXLOhey7GM55YGAgvvGNb8ShQ4eirKzsiufNxHlf91dWbrrppliwYMGEVxTOnDkzocYuWr58+aTjS0pKYunSpbO21pkynT1PZu3atfHOO+/M9PKSMtfPeqbM1bN+7LHH4sCBA3H48OFYuXLllGOL6awL2fdk5tp5L1q0KD75yU9GfX19tLe3x1133RXf/e53Jx1bTOdcyL4nM9fO+bXXXoszZ85EXV1dlJSURElJSXR3d8fTTz8dJSUlceHChQlzZuq8r3usLFq0KOrq6qKzs3Pc/c7OzrjnnnsmndPQ0DBh/KFDh6K+vj4WLlw4a2udKdPZ82T6+vqiqqpqppeXlLl+1jNlrp11lmXx6KOPxr59++InP/lJ1NTUXHZOMZz1dPY9mbl23h+WZVmMjo5O+m/FcM6XMtW+JzPXzrmpqSnefPPNOHbsWP6qr6+PL33pS3Hs2LFYsGDBhDkzdt4FfRx3lrz44ovZwoULsz179mS/+MUvsi1btmS//du/nb377rtZlmXZE088kT388MP58f/6r/+a/dZv/Va2devW7Be/+EW2Z8+ebOHChdnf//3fX68tFKzQPX/nO9/J9u/fn/3yl7/M/umf/il74oknsojIOjo6rtcWpuX999/P+vr6sr6+viwism9/+9tZX19fdurUqSzLivOsC91zMZz117/+9ayioiLr6urKBgcH89d//dd/5ccU41lPZ99z/bzb2tqynp6e7OTJk9kbb7yRbd++PbvhhhuyQ4cOZVlWnOecZYXve66f86V8+KeBZuu8k4iVLMuyZ555JrvllluyRYsWZZ/5zGfG/ajfxo0bs89+9rPjxnd1dWVr1qzJFi1alN16663Zzp07r/GKr14he/6Lv/iL7BOf+ERWVlaWfexjH8vuvffe7ODBg9dh1Vfn4o/vffjauHFjlmXFedaF7rkYznqy/UZE9txzz+XHFONZT2ffc/28//iP/zj//9jHP/7xrKmpKf8NO8uK85yzrPB9z/VzvpQPx8psnXcuy/7/J10AABJ03T+zAgAwFbECACRNrAAASRMrAEDSxAoAkDSxAgAkTawAAEkTKwBA0sQKAJA0sQIAJE2sAABJ+3/sLUbQapYZOAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "world = World(3,4)\n",
    "world.setReward(2, 3, 1.0) #Goal state\n",
    "world.setReward(1, 1, -1.0) #Fear region\n",
    "world.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d682ba-3c90-4a9a-8a25-87d006bcf75a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "a5b9723b-0141-4525-a7c5-d0d4dc51ea04",
   "metadata": {},
   "source": [
    "# Defining the Agent_SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6878ae0-5900-4306-9d24-9e3e639b92c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_SARSA(object):\n",
    "    def __init__(self, world):\n",
    "        self.world = world\n",
    "        self.numOfActions = self.world.getNumOfActions()\n",
    "        self.numOfStates = self.world.getNumOfStates()\n",
    "        self.Q = np.random.uniform(0.0,0.01,(self.numOfStates,self.numOfActions))\n",
    "        self.alpha = 0.7\n",
    "        self.gamma = 0.4\n",
    "        self.epsilon = 0.25\n",
    "\n",
    "    # epsilon-greedy action selection\n",
    "    def actionSelection(self, state):\n",
    "         # To do\n",
    "        \n",
    "        # Add the algorithm that the agent choose an action based on epsilon-greedy action selection    \n",
    "        return action\n",
    "\n",
    "    def train(self, iter):\n",
    "        for itr in range(iter):\n",
    "\n",
    "            state = int(np.random.randint(0,self.numOfStates))\n",
    "            self.world.resetAgent(state)\n",
    "\n",
    "            # choose action\n",
    "            a = self.actionSelection(state)\n",
    "            expisode = True\n",
    "\n",
    "            while expisode:\n",
    "                # perform action\n",
    "                self.world.move(a)\n",
    "                # look for reward\n",
    "                reward = self.world.getReward()\n",
    "                state_new = int(self.world.getState())\n",
    "                # new action\n",
    "                a_new = self.actionSelection(state_new)\n",
    "\n",
    "                # To do\n",
    "                # calculate q value  and do the updating\n",
    "\n",
    "                if reward == 1.0:\n",
    "                    self.Q[state_new,:] = 0\n",
    "                    expisode = False\n",
    "\n",
    "        print(self.Q)\n",
    "        return self.Q\n",
    "\n",
    "    def plotQValues(self):\n",
    "        plt.rcParams.update({'font.size': 18})\n",
    "        plt.imshow(self.Q, cmap='Oranges', interpolation='nearest', aspect='auto')\n",
    "        plt.colorbar()\n",
    "        plt.title(\"Q-values\")\n",
    "        plt.xlabel(\"Actions\")\n",
    "        plt.ylabel(\"States\")\n",
    "        plt.xticks(np.arange(4), ('Down', 'Up', 'Right', 'Left'))\n",
    "        plt.yticks(np.arange(self.numOfStates), np.arange(self.numOfStates))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2a730ca-7263-4b84-989c-ee360b524880",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'action' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m learner_SARSA \u001b[38;5;241m=\u001b[39m Agent_SARSA(world)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#Section 3.d.\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m learner_SARSA\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#Section 3.e.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m learner_SARSA\u001b[38;5;241m.\u001b[39mplotQValues()\n",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m, in \u001b[0;36mAgent_SARSA.train\u001b[0;34m(self, iter)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld\u001b[38;5;241m.\u001b[39mresetAgent(state)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# choose action\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactionSelection(state)\n\u001b[1;32m     25\u001b[0m expisode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m expisode:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# perform action\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m, in \u001b[0;36mAgent_SARSA.actionSelection\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mactionSelection\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m     13\u001b[0m      \u001b[38;5;66;03m# To do\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Add the algorithm that the agent choose an action based on epsilon-greedy action selection    \u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "\u001b[0;31mNameError\u001b[0m: name 'action' is not defined"
     ]
    }
   ],
   "source": [
    "#Section 3.a.\n",
    "world = World(3,4)\n",
    "\n",
    "#Section 3.b.\n",
    "world.setReward(2, 3, 1.0) #Goal state\n",
    "world.setReward(1, 1, -1.0) #Fear region\n",
    "\n",
    "#Section 3.c.\n",
    "learner_SARSA = Agent_SARSA(world)\n",
    "\n",
    "#Section 3.d.\n",
    "learner_SARSA.train(1000)\n",
    "\n",
    "#Section 3.e.\n",
    "learner_SARSA.plotQValues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e11b5-2242-4452-b5d3-6da37553e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_path(agent, world):\n",
    "    done = True\n",
    "    steps = 0\n",
    "\n",
    "    # Start with a random state\n",
    "    state = int(np.random.randint(0, agent.numOfStates))\n",
    "    world.resetAgent(state)\n",
    "\n",
    "    while done:\n",
    "        world.display() \n",
    "        \n",
    "        #To do\n",
    "        # Choose the action based on the Q-table\n",
    "        print('Action taken:', )\n",
    "        print('Q-table values for all actions in the state:',)\n",
    "        world.move()\n",
    "\n",
    "        # Get reward and new state\n",
    "        reward = world.getReward()\n",
    "        state = int(world.getState())\n",
    "        steps += 1\n",
    "\n",
    "        # Check if the episode is done\n",
    "        if reward == 1.0:\n",
    "            done = False\n",
    "\n",
    "    world.display()  # Final visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f0b904-1b4d-4c50-8774-b75835776187",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_path(learner, world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5c81a4-2118-47cb-8c53-bf11c501544d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "779cc945-4b96-45e1-ae11-2fe185cce695",
   "metadata": {},
   "source": [
    "Extra activity: Can you develope an agent works based on Q-learning? Can you identify the difference between SARSA and \n",
    "Q-learning algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e614f4c1-7576-4550-8e1e-1591486d7d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_Q_learning(object):\n",
    "    def __init__(self, world):\n",
    "        self.world = world\n",
    "        self.numOfActions = self.world.getNumOfActions()\n",
    "        self.numOfStates = self.world.getNumOfStates()\n",
    "        self.Q = np.random.uniform(0.0, 0.01, (self.numOfStates, self.numOfActions))\n",
    "        self.alpha = 0.7\n",
    "        self.gamma = 0.4\n",
    "        self.epsilon = 0.25\n",
    "\n",
    "    # epsilon-greedy action selection\n",
    "    def actionSelection(self, state):\n",
    "         # To do\n",
    "        # Add the algorithm that the agent choose an action based on that    \n",
    "        return action\n",
    "\n",
    "    def train(self, iter):\n",
    "        for itr in range(iter):\n",
    "            state = int(np.random.randint(0, self.numOfStates))\n",
    "            self.world.resetAgent(state)\n",
    "\n",
    "            expisode = True\n",
    "\n",
    "            while expisode:\n",
    "                # choose action\n",
    "                a = self.actionSelection(state)\n",
    "\n",
    "                # perform action\n",
    "                self.world.move(a)\n",
    "\n",
    "                # look for reward\n",
    "                reward = self.world.getReward()\n",
    "                state_new = int(self.world.getState())\n",
    "                \n",
    "                #To do\n",
    "                # update Q-values (using max Q-value for next state)\n",
    "\n",
    "\n",
    "                if reward == 1.0:\n",
    "                    self.Q[state_new,:] = 0\n",
    "                    expisode = False\n",
    "\n",
    "        print(self.Q)\n",
    "        return self.Q\n",
    "\n",
    "    def plotQValues(self):\n",
    "        plt.rcParams.update({'font.size': 18})\n",
    "        plt.imshow(self.Q, cmap='Oranges', interpolation='nearest', aspect='auto')\n",
    "        plt.colorbar()\n",
    "        plt.title(\"Q-values\")\n",
    "        plt.xlabel(\"Actions\")\n",
    "        plt.ylabel(\"States\")\n",
    "        plt.xticks(np.arange(4), ('Down', 'Up', 'Right', 'Left'))\n",
    "        plt.yticks(np.arange(self.numOfStates), np.arange(self.numOfStates))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba86353-b65c-420e-a428-6de811a1fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "world = World(3,4)\n",
    "\n",
    "#Section 3.b.\n",
    "world.setReward(2, 3, 1.0) #Goal state\n",
    "world.setReward(1, 1, -1.0) #Fear region\n",
    "\n",
    "#Section 3.c.\n",
    "learner_Q_learning = Agent_Q_learning(world)\n",
    "\n",
    "#Section 3.d.\n",
    "learner_Q_learning.train(1000)\n",
    "\n",
    "#Section 3.e.\n",
    "learner_Q_learning.plotQValues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ba5590-0007-4ba9-8dff-65ba1225db3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_path(learner_Q_learning , world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d562cca-e69b-4199-8e91-0e31b1b18b36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
